<!doctype html>
<html>
    <head>
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
        <meta name="description" content="keep coding, keep foolish"/>
        <meta name="keywords" content="代码,数学,数据挖掘,机器学习,算法,编译器,分布式,PHP,Nginx"/>
        <meta name="author" content="张洋"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
        <title>CodingLabs - 统计学习理论的数理基础
</title>
        <link rel="stylesheet" href="/assets/vendor/normalize.css"/>
        <link rel="stylesheet" href="/assets/vendor/google-code-prettify/prettify-solarized-dark.css"/>
        <link rel="stylesheet" href="/assets/themes/default/main.css"/>
        <link rel="shortcut icon" href="/fav.ico"/>
        <script type="text/javascript" src="/assets/vendor/google-code-prettify/prettify.js"></script>
        <script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3F75dfda8ff8a93fa0b94adeb17da37a2f' type='text/javascript'%3E%3C/script%3E"));
</script>

    </head>
    <body onload="prettyPrint()">
    <div id="header">
        <div id="header-inner">
            <div id="title"><a href="/">CodingLabs</a></div>
            <div id="subtitle">keep coding, keep foolish</div>
        </div>
    </div>

<div id="main">
    <div id="main-inner">
        <div id="topnav">
            <ul>
                <li><a href="/">首页</a></li>
                <li class="sep"> | </li>
                <li><a href="/tag.html">标签</a></li>
                
<li class="sep"> | </li>
<li><a href="/pages/about-me.html" target="_self">关于我</a></li>

<li class="sep"> | </li>
<li><a href="/rss.xml" target="_self">+订阅</a></li>

<li class="sep"> | </li>
<li><a href="http://weibo.com/ericzhangbuaa" target="_blank">微博</a></li>


            </ul>
            <div style="clear:both;"></div>
        </div>
        <div id="article-title">
            <a href="/articles/statistical-learning-theory.html">统计学习理论的数理基础</a>
        </div>
        <div id="article-meta">
            作者 张洋 | 发布于 2018-09-10
        </div>
        <div id="article-tags">
        
        <a class="tag" href="/tag.html#机器学习">机器学习</a>
        
        <a class="tag" href="/tag.html#算法">算法</a>
        
        <a class="tag" href="/tag.html#数学">数学</a>
        
        </div>
        <div id="article-content">
            <p>统计学习（Statistical learning）是目前人工智能领域最为活跃的一个分支，其理论基础是统计学习理论（Statistical learning theory，以下有时会简称SLT）：一种以数理统计为数学基础，研究是否可以以及如何从经验数据中学习普遍概念的理论。</p>
<p>目前诸多机器学习相关的初级书籍中，重点都放在了对各种模型的探讨，而对统计学习理论部分，要么是不涉及，要么是简要几页草草描述一下，这种不成体系的论述无法令读者尤其是机器学习的初学者对统计学习的基础理论形成系统化认知。</p>
<p>所以我阅读了一些SLT相关的论文，并对SLT的基础部分进行了一个系统化学习，这篇文章是我对SLT数理基础的整理和总结。</p>
<p>这篇文章目标是对于统计学习理论进行一个概述，在这个概述中，将始终保持简洁及易读，并尽量将SLT基础部分的系统脉络梳理清楚，给出SLT的一些基础但十分重要的结论。</p>
<p>虽然不希望数学公式成为各位阅读本文的障碍，但是想要完全抛弃数学语言，又可以明确阐述SLT是不可能的，因此文章中会不可避免存在一些相对严格的数学描述、公式及证明。读者只要具有本科工科的数学水平，即可以无障碍阅读此文。</p>
<!-- toc -->

<ul>
<li><a href="#1-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9A%E4%B9%89%E5%8F%8A%E6%A1%86%E6%9E%B6">1 统计学习的定义及框架</a><ul>
<li><a href="#11-%E7%9B%AE%E6%A0%87">1.1 目标</a></li>
<li><a href="#12-%E6%9C%80%E4%BC%98%E8%A7%A3%E7%A4%BA%E4%BE%8B">1.2 最优解示例</a></li>
<li><a href="#13-%E9%80%9A%E7%94%A8%E6%9C%80%E4%BC%98%E8%A7%A3">1.3 通用最优解</a><ul>
<li><a href="#131-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E7%9A%84%E6%9C%80%E4%BC%98%E8%A7%A3">1.3.1 回归问题的最优解</a></li>
<li><a href="#132-%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E6%9C%80%E4%BC%98%E8%A7%A3">1.3.2 二分类问题的最优解</a></li>
</ul>
</li>
<li><a href="#14-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0">1.4 统计学习</a></li>
</ul>
</li>
<li><a href="#2-%E4%B8%80%E8%87%B4%E6%80%A7">2 一致性</a><ul>
<li><a href="#21-%E5%AF%B9%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%E7%9A%84%E6%80%9D%E8%80%83">2.1 对假设空间的思考</a></li>
<li><a href="#22-%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%9D%87%E8%A1%A1">2.2 偏差-方差均衡</a></li>
<li><a href="#23-%E4%B8%80%E8%87%B4%E6%80%A7">2.3 一致性</a></li>
</ul>
</li>
<li><a href="#3-%E5%85%A8%E5%B1%80%E4%B8%80%E8%87%B4%E7%AE%97%E6%B3%95">3 全局一致算法</a><ul>
<li><a href="#31-k-nearest-neighbor%E7%AE%97%E6%B3%95%E5%AE%9A%E4%B9%89">3.1 K-nearest neighbor算法定义</a></li>
<li><a href="#32-knn%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7">3.2 kNN的一致性</a></li>
</ul>
</li>
<li><a href="#4-%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96">4 经验风险最小化</a><ul>
<li><a href="#41-%E5%AE%9A%E4%B9%89">4.1 定义</a></li>
<li><a href="#42-%E5%AF%B9erm%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%88%9D%E6%AD%A5%E6%8E%A2%E8%AE%A8">4.2 对ERM一致性的初步探讨</a></li>
<li><a href="#43-%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%E4%B8%8E%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B">4.3 假设空间与一致收敛</a><ul>
<li><a href="#431-%E5%8D%95%E4%B8%80%E5%87%BD%E6%95%B0%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%E4%B8%8E%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B">4.3.1 单一函数假设空间与大数定律</a></li>
<li><a href="#432-%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9Buniform-convergence">4.3.2 一致收敛（Uniform Convergence）</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#5-%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4%E5%BA%A6%E9%87%8F">5 假设空间度量</a><ul>
<li><a href="#51-%E6%9C%89%E9%99%90%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4">5.1 有限假设空间</a></li>
<li><a href="#52-%E6%97%A0%E9%99%90%E5%81%87%E8%AE%BE%E7%A9%BA%E9%97%B4">5.2 无限假设空间</a><ul>
<li><a href="#521-symmetrization">5.2.1 Symmetrization</a></li>
<li><a href="#522-shattering-coefficient">5.2.2 Shattering Coefficient</a></li>
<li><a href="#523-%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E4%B8%8A%E7%95%8C">5.2.3 一致收敛上界</a></li>
<li><a href="#524-vc%E7%BB%B4">5.2.4 VC维</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#6-%E5%A4%A9%E4%B8%8B%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86">6 天下没有免费午餐定理</a></li>
</ul>
<!-- toc stop -->


<h1 id="1-统计学习的定义及框架">1 统计学习的定义及框架</h1>
<h2 id="11-目标">1.1 目标</h2>
<p>我们先来相对严格的描述，统计学习的目标是什么。</p>
<p>现设存在以下实体：</p>
<ol>
<li>集合$X$，称为输入空间，集合$Y$，称为输出空间</li>
<li>$X$与$Y$的联合概率分布$P(X,Y)$</li>
<li>$X$到$Y$的映射，$f(x)=y$，其中$x\in X, y\in Y$</li>
<li>定义在$X$，$Y$及$f$上的函数$L(x,y,f)\rightarrow \mathbb{R}$，其中$\mathbb{R}$表示实数集合</li>
</ol>
<p>统计学习的终极目标是<strong>找一个映射$f$，使得$L$的期望最小</strong>。所以，统计学习本质上是一个最优化问题，用数学语言描述，统计学习的目标找到下面的映射：
$$
f^\star=\mathop{\arg\min}_{f}{E(L(x,y,f))}
$$</p>
<h2 id="12-最优解示例">1.2 最优解示例</h2>
<p>注意假设我们知道$X$，$Y$，$P$和$L$的具体形式，那么统计学习则是根本不必要的，因为这本身变成了一个数学上的最优化问题，我们先忽略这个问题，并通过几个例子，建立对这个理论目标的直观认识。</p>
<blockquote>
<p>例1：$L$为常数函数$L(x,y,f)=0$</p>
</blockquote>
<p>根据期望的定义可知，此情况下$L$的期望$E(L(x,y,f))$亦退化为常函数恒等于0，因此问题变得十分trivial，只要随便取一个映射，如$f(x)=0$，即是目标映射</p>
<blockquote>
<p>例2：$X=\mathbb{R}$，$Y=\mathbb{R}$，$L(x,y,f)=(y-f(x))^2$，且已知对于任意$x\in X$有$y=x^2$</p>
</blockquote>
<p>此时，输入和输出之间的关系是完全确定的，不存在随机性。由$L$的定义可知$L\ge0$，因此只要取$f(x)=x^2$即可令$L$处处为0，因此期望也自然是0。此时$f(x)=x^2$就是最优解。</p>
<blockquote>
<p>例3：$X=\mathbb{R}$，$Y=\mathbb{R}$，$L(x,y,f)=(y-f(x))^2$，且已知对于任意$x\in X$有$y=x^2+\epsilon$，其中$\epsilon\sim N(\mu,\sigma^2)$，即$\epsilon$服从均值为$\mu$，方差为$\sigma^2$的正态分布</p>
</blockquote>
<p>这里和例2唯一的不同是，输出中多了一个随机变量。此时：
$$
E(L(x,y,f))=E((y-f(x))^2)
$$
取$f(x)=x^2+\mu$，则：
$$
\begin{align}
E(L(x,y,f)) &= E((x^2+\epsilon-x^2-\mu)^2) \\
&= E((\epsilon-\mu)^2) \\
&= E(\epsilon^2+\mu^2-2\mu\epsilon) \\
&= E(\epsilon^2)+E(\mu^2)-2E(\mu)E(\epsilon) \\
&= E^2(\epsilon)+Var(\epsilon) + \mu^2 - 2\mu^2 \\
&= \mu^2+\sigma^2+\mu^2-2\mu^2 \\
&= \sigma^2
\end{align}
$$
可以证明这是$E(L(x,y,f))$的最小值，所以$f(x)=x^2+\mu$是此时的最优解。</p>
<blockquote>
<p>例4：$X=\{0,1\}$，$Y=\{0,1\}$，当$f(x)=y$，$L(x,y,f)=0$，否则$L(x,y,f)=1$，联合概率分布$P$取值如下：$P(X=0,Y=0)=0.1$，$P(X=0,Y=1)=0.3$，$P(X=1,Y=0)=0.4$，$P(X=1,Y=1)=0.2$</p>
</blockquote>
<p>这是一个离散情况，我们现在不经证明给出起最优映射为$f(x)=1-x$，此时$L$的期望为：
$$
E(L(x,y,f))=0.1\times1+0.3\times0+0.4\times0+0.2\times1=0.3
$$</p>
<h2 id="13-通用最优解">1.3 通用最优解</h2>
<p>上面举了几个在各项条件已知的情况下，最优映射的例子，可以看到，随着$X$，$Y$，$L$，$P$的不同，最优解的形式也各不相同。那么我们自然会有一个疑问：是否存在一个通用最优解公式，对于任意的$X$，$Y$，$L$，$P$，均可以套用公式得到最优解？答案是存在。</p>
<p>但是直接在如此抽象的定义域上讨论通用最优解，会使得整个推理过于抽象，所以我们对定义域进行一定的限制，将重点放在以下两类常见的统计学习问题：</p>
<ul>
<li>回归问题（Regression）：$X=\mathbb{R}^m$，$Y=\mathbb{R}$，$L(x,y,f)=(y-f(x))^2$</li>
<li>二分类问题（Binary Classification）：$X=\mathbb{R}^m$，$Y=\{0,1\}$，$L(x,y,f)=|y-f(x)|$</li>
</ul>
<p>下面分别分析两类问题的最优解。</p>
<h3 id="131-回归问题的最优解">1.3.1 回归问题的最优解</h3>
<p>在上述回归问题中，我们要求解的目标变成了：
$$
f^\star=\mathop{\arg\min}_f{E(L(x,y,f))}=\mathop{\arg\min}_f\int_Y\int_X(y-f(x))^2P(x,y)dxdy
$$
具体导出最优解的数学过程有点繁琐，所以这里我们用一个非严格但相对直观的方式，推导一下最优解。对严格数学推导过程感兴趣的同学可以自行推导，或参考资料。</p>
<p>首先，由上述公式可以看出，这里的期望值是一个大于等于0的值，且显然$(y-f(x))^2\ge0$。因此我们直观上能感受到，最优解$f^\star$应该使得$f^\star(x)$在任何地方都尽量接近$y$，这样才能让$(y-f(x))^2$尽可能小。</p>
<p>但是由于一般情况下我们认为$y$对$x$不是确定函数（否则这个函数就直接是最优解了），所以我们用条件概率刻画这个关系，任意给定一对$x\in X$，其对应的$y$服从：
$$
y\sim P(Y|X=x)
$$
而这个条件概率，可以对联合概率边缘化导出：
$$
P(Y|X=x)=P(x,Y)
$$
而要想令$(y-f(x))^2$尽可能小，直觉上我们可以让$f^\star(x)$取$X=x$是$y$的条件期望（实际上数学严格证明结论也是如此）：
$$
f^\star(x)=E_{Y|X}(Y|X=x)=\int_YyP(y|X=x)dy
$$
也就是说<strong>回归问题的最优解是输入值的条件期望</strong>。</p>
<h3 id="132-二分类问题的最优解">1.3.2 二分类问题的最优解</h3>
<p>二分类问题的最优解分析与回归问题类似，但是要简单很多，这得益于二分类问题的输出空间要更简单一些。下面具体看一下：</p>
<p>与分析回归类似，我们先代入已知条件，写出优化目标：
$$
f^\star=\mathop{\arg\min}_f{E(L(x,y,f))}=\mathop{\arg\min}_f\{\int_X|0-f(x)|P(x,0)dx+\int_X|1-f(x)|P(x,1)dx\}
$$
上面的期望可以将联合概率分布改写为条件概率分布形式：
$$
E(L(x,y,f))=\int_X|0-f(x)|P(Y=0|x)P(x)dx+\int_X|1-f(x)|P(Y=1|x)P(x)dx
$$</p>
<p>同样我们来非严格的分析一下这个式子如何取最小值。注意这里$f(x)$可能的取值只有0和1，所以$|0-f(x)|$和$|1-f(x)|$的取值一定是一个0一个1，所以按直觉来说一个合理的推断是无论何时，我们希望让被积函数部分尽可能小，具体来说：</p>
<ol>
<li>如果$P(Y=0|x)P(x) &gt; P(Y=1|x)P(x)$，我们希望$|0-f(x)|=0$且$|1-f(x)|=1$，即令$f(x)=0$</li>
<li>如果$P(Y=0|x)P(x) &lt; P(Y=1|x)P(x)$，我们希望$|0-f(x)|=1$且$|1-f(x)|=0$，即令$f(x)=1$</li>
</ol>
<p>另外我们注意到$P(x)$是一个大于等于0的常数，所以我们只要在$P(Y=0|x)$与$P(Y=1|x)$选择较大的即可，即：
$$
f^\star(x)=\mathop{\arg\max}_y{P(Y=y|X=x)}
$$
<strong>以上最优解叫做贝叶斯分类器，是二分类问题的理论最优分类器，也是平均意义下统计学习所能达到的分类器上限</strong>。</p>
<h2 id="14-统计学习">1.4 统计学习</h2>
<p>下面我们严格定义统计学习。为了简单起见，从现在起，我们所有的讨论都围绕二分类问题展开，所得到的各种结论，理论上都可以推广到一般化的问题，但是在这里就不再从一般意义上进行推导，而是默认将问题限定在二分类问题。</p>
<p>首先，我们给上面到处都用到的那个期望$E(L(x,y,f))$起一个名字：风险，用$R$表示。注意在其他条件已知的情形下，$R$是$f$的函数，即：
$$
R(f)=E(L(x,y,f))
$$
因此我们上述目标可以简化成：
$$
f^\star=\mathop{\arg\min}_{f}R(f)
$$
上文可以知道，如果我们知道联合概率分布$P(X,Y)$，则可以通过数学计算直接导出最优解：贝叶斯分类器。因此也就不需要统计学习什么的了，但是现实情况是，我们往往不知道，也无法通过什么方法观测到$P(X,Y)$，而只可以观测到一个可数但无穷（现实中往往是又穷的，但这里我们暂时放宽这个条件）的<strong>独立无偏</strong>样本：$D_n=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\},n\rightarrow\infty$，我们是否有某种可靠的方式，去得到或逼近贝叶斯分类器。</p>
<p>严格来说，（二分类）统计学习是这样一个问题：</p>
<hr>
<p><strong>统计学习</strong></p>
<p>已知输入空间$X=\mathbb{R}^m$，输出空间$Y=\{0,1\}$，损失函数$L(x,y,f)=|y-f(x)|$。另存在一固定但未知且不可直接观测联合概率分布$P(X,Y)$，以及可数但任意大的iid（独立同分布）抽样$D_n=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\},n\rightarrow\infty$。设$f_b$为$P$下的贝叶斯分类器。</p>
<p>现给出一个函数空间$F$（称为假设空间），和从$F$中选择分类器算法，使得当$n\rightarrow\infty$时，算法从$F$中选择的分类器的风险依概率收敛到贝叶斯分类器的风险，即对于任意$\epsilon&gt;0$，由算法选择的$f\in F$满足：
$$
P(|R(f)-R(f_b)|&gt;\epsilon)\rightarrow0,\text{when }n\rightarrow\infty
$$</p>
<hr>
<p>注意，以上定义是一个非常严苛的定义，在此顶一下，我们要求我们的算法在样本无限多时能以任意大的概率和任意小的差距逼近最优分类器且对联合概率分布没有任何假设。这是我们最理想的统计学习，但实际中，由于达到这个目标非常困难，所以我们可能会退而求其次寻求一些更宽松的目标。</p>
<p>这一章节，我们通过数学方式严格定义了统计学习，并给出了最理想的情况。下一章节，我们进一步统计学习的一些细节问题。</p>
<h1 id="2-一致性">2 一致性</h1>
<h2 id="21-对假设空间的思考">2.1 对假设空间的思考</h2>
<p>上面说道，我们的算法是从一个假设空间$F$中选择一个假设（函数），由此引出一个问题，不同的假设空间，对于是否能达到上述目标是有影响的。考虑下图中的两个假设空间：</p>
<p><img src="../uploads/pictures/statistical-learning-theory/slt01.png" alt="slt01"></p>
<p>其中All sapce是所有可能的分类器集合，橙色的分类器是贝叶斯分类器。可以看到，假设空间1包含了贝叶斯分类器，而假设空间2没有包含。那么，理论上任何算法都不可能从假设空间2中学到贝叶斯分类器（假设不存在与贝叶斯分类器的等效分类器），所以，如果我们不能保证所采用的假设空间包含贝叶斯分类器，就不可能达到上述的学习目标。</p>
<p>同时，由于我们对联合概率分布$P$一无所知，现实中很难有办法保证我们选择的假设空间一定包含$f_b$。</p>
<p>那么我们换个思路，能不能直接用All space作为假设空间。理论上是可以的，而且也确实有这种算法（例如KNN）。这样做虽然可以理论上直接以$f_b$为优化目标，但只有在样本真的是无限的情况下才可能，而且想要尽可能逼近$f_b$，对样本量的需求和对计算量的需求都会膨胀的十分迅速，因此实践中受现实条件所限，有可能无法使用。</p>
<h2 id="22-偏差-方差均衡">2.2 偏差-方差均衡</h2>
<p>另一类实践中更常用的方法是经验风险最小化（Empirical Risk Minimization，以下简称ERM），下面会有对ERM的单独讨论。这里只是简单说一下，ERM中存在偏差-方差均衡问题。为了解释这个，我们看下面的图：</p>
<p><img src="../uploads/pictures/statistical-learning-theory/slt02.png" alt="slt02"></p>
<p>橙色点是贝叶斯分类器$f_b$，绿色点是我们的假设空间中最优的分类器$f_F$，其数学定义为：
$$
f_F=\mathop{\arg\min}_{f\in F}(R(f_b)-R(f))
$$
灰色点是我们的学习算法在观察了$n$个样本后得到的分类器$f_n$，这里假设$f_b\notin F$。此时相较于最优分类器，我们的整体差距为：
$$
R(f_b)-R(f_n)=(R(f_n)-R(f_F))+(R(f_F)-R(f_b))
$$
其中前一部分叫做方差（Variance），含义是我们学到的分类器和假设空间中最优分类器的差距，后者叫做偏差（Bias），表示假设空间中最优分类器与贝叶斯分类器的差距。</p>
<p>我们当然希望两者可以同时被优化，但是后面的量化分析会提到，在ERM学习框架下，两者是矛盾的，为了缩小偏差，我们需要一个更大的假设空间，而更大的假设空间则意味着更大的偏差，甚至有可能导致ERM的学习过程不收敛（理论上不管多少样本也无法学到$f_F$），因此为了更小的方差，ERM希望有个一小的$F$，而这又往往意味着大的偏差。</p>
<p>总而言之，ERM框架下，一般来说实践中无法同时将方差和偏差无限优化，需要在两者间做一个权衡（样本无限时理论上偏差和方差可以同时收敛到0，但实践中往往不可能拥有无限样本）。</p>
<p>这里先简要给出这样一个结论，后面对ERM框架的定量分析会说明为什么会存在这个问题。</p>
<h2 id="23-一致性">2.3 一致性</h2>
<p>有了上述内容，现在我们可以定义一致性了，对一致性的严格描述是后续具体算法定量分析的基础。这里我们直接给出三种一致性的定义：</p>
<p><strong>一致（Consistent）</strong></p>
<p>在假设空间$F$和联合概率分布$P(X,Y)$固定的情况下，如果随着$n$趋向于无穷大，$f_n$依概率收敛于$f_F$，即对于任意$\epsilon&gt;0$：
$$
P(R(f_n)-R(f_F)&gt;\epsilon)\rightarrow 0,\text{when }n\rightarrow\infty
$$
则称学习算法一致。</p>
<p><strong>贝叶斯一致（Bayes-consistent）</strong></p>
<p>在假设空间$F$和联合概率分布$P(X,Y)$固定的情况下，如果随着$n$趋向于无穷大，$f_n$依概率收敛于$f_b$，即对于任意$\epsilon&gt;0$：
$$
P(R(f_n)-R(f_b)&gt;\epsilon)\rightarrow 0,\text{when }n\rightarrow\infty
$$
则称学习算法贝叶斯一致。</p>
<p><strong>全局一致（Universally consistent）</strong></p>
<p>在给定假设空间$F$的情况下，若不论联合概率分布$P(X,Y)$是什么分布，学习算法均贝叶斯一致，则称此算法全局一致。</p>
<p>可以看到，三种一致从上到下是越来越苛刻的，所以也越来越难达到。我们在1.4中给出的统计学习定义，是按照全局一致来定义的，这是最理想的情况。在实践中，很多时候并无法达到全局一致，所以实际中能达到贝叶斯一致甚至仅仅能做到一致的算法，也是十分有用的（同时也更加简单且节省资源）。</p>
<h1 id="3-全局一致算法">3 全局一致算法</h1>
<p>长期以来，人们并不知道是否存在全局一致的统计学习算法。直到1977年，有人从数学上证明了K-nearest neighbor（kNN）算法（服从某些条件时）是全局一致的。</p>
<p>我们现在来讨论kNN和其一致性问题。</p>
<h2 id="31-k-nearest-neighbor算法定义">3.1 K-nearest neighbor算法定义</h2>
<p>对于二分类问题，kNN可以这样描述：</p>
<hr>
<p><strong>K-nearest neighbor算法（用于二分类的）</strong></p>
<p>设$d$是定义在$X$空间上的距离函数，其满足如下条件：</p>
<ul>
<li>任一点到自己的距离为零，即$d(x,x)=0$</li>
<li>$d$的取值非负</li>
<li>满足三角不等式，即：$d(x_1,x_2)\le d(x_1,x_3)+d(x_3,x_2)$</li>
</ul>
<p>kNN这样做分类：</p>
<p>对于待分类点$x\in X$，取$D_n$中与$x$距离最小的k个点的类别$Y_k=\{y_1,y_2,...,y_k\}$，以$Y_k$的众数（mode）为$x$的取值。</p>
<hr>
<p>用白话说，就是找到$x$最近的k个样本，看这k个样本多数是0还是1，然后用多数表决的方式，为$x$分类。</p>
<p>不难证明，kNN是在忽略联合分布的情况下，直接去逼近贝叶斯分类器的方法。那么这是否说明kNN一定是全局一致呢？</p>
<h2 id="32-knn的一致性">3.2 kNN的一致性</h2>
<p>我们先考虑最简单k=1的情况。考虑下面的例子：</p>
<blockquote>
<p>例5：设输入空间$X\sim Uni(0,1)$，即在0到1的区间服从均匀分布，$Y$是一个与$X$独立的随机变量，且取0的概率为0.1，取1的概率为0.9。使用1NN分类器进行分类，使用欧氏距离为距离函数，求贝叶斯分类器及1NN分类器的风险各是多少。</p>
</blockquote>
<p>先来考虑贝叶斯风险，由于$Y$独立于$X$，且取值1的概率更大，因此贝叶斯分类器为$f_b(x)=1$，此时贝叶斯风险为：
$$
R(f_b)=1\times0.1+0\times0.9=0.1
$$
而使用1NN分类器时，不管样本多大，离$x$最近的样本也服从取0的概率为0.1，取1的概率为0.9，我们设离$x$最近的样本为$x_d$因此，1NN分类器的风险为：
$$
R(f_{1nn})=P(y=0)\times P(x_d=1)+P(y=1)\times P(x_d=0)=0.1\times 0.9 + 0.9\times 0.1=0.18
$$
这个风险与样本集的数量无关。因此，1NN分类器在此情况下，不一致。</p>
<p>使用同样的思路，我们可以证明，<strong>对于任意的有限的k，kNN算法均不一致</strong>。</p>
<p>至此我们得到了一个kNN必要条件：若要kNN分类器一致，一个必要条件是$k\rightarrow\infty$。</p>
<p>那么这个条件是否也是一个充分条件呢？并不是。Stone在1977年关于kNN全局一致证明的论文中，给出了如下充要条件：</p>
<p><strong>kNN算法全局一致，当且仅当，随着$n\rightarrow \infty$，$k\rightarrow \infty$，且$k/n\rightarrow 0$</strong></p>
<p>由此可以看成，kNN虽然是一个理论上全局一致的算法，但是实际中受限于训练样本的数量，其实很难做到适用于各种场景的万金油。否则，我们也不用研究其他算法，全都用kNN就好了。</p>
<p>同时可以从上面看到，如果想要kNN发挥较好的效果，除了样本量$n$要足够大外，$k$的选择也尤为重要，根据理论分析，较好的选择是$k\sim log(n)$。如果k过小，例如1NN，此时方差很大，模型很容易过拟合，但如果k特别大，例如极端情况，$k=n$，此时分类器对于所有新样本均预测为同样的值（样本集的多数），导致欠拟合。</p>
<h1 id="4-经验风险最小化">4 经验风险最小化</h1>
<p>从上面分析可以看出，kNN算法虽然理论上可以达到全局一致，但是受限于现实中样本数量，并不是处处通用。下面讨论另一种统计学习准则：经验风险最小化。</p>
<h2 id="41-定义">4.1 定义</h2>
<p>所谓经验风险最小化（Empirical Risk Minimization，简称ERM），就是选择分类器$f_n$，使得分类结果在训练样本集上风险最小。为了形式化定义ERM，我们先定义经验风险：</p>
<p>设有样本集$D_n=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，分类器$f$的经验风险定义为：
$$
R_{emp}(f)=\frac{1}{n}\sum_{i=1}^n{L(x_i,y_i,f)}
$$
经验风险最小化就是选择$f_n$，使得：
$$
f_n=\mathop{\arg\min}_{f\in F}R_{emp}(f)
$$
直觉上这是很合理的，如果训练样本来自总体的无偏独立抽样，那么感觉只要最小化经验风险，应该也可以得到一个总体风险很小的分类器。但这毕竟是直觉，下面我们从数学上严格的分析一下ERM是否能真的如我们所愿。</p>
<h2 id="42-对erm一致性的初步探讨">4.2 对ERM一致性的初步探讨</h2>
<p>首先我们来考察ERM的一致性，这里我们先不要求全局一致或贝叶斯一致，只要求最宽松的一致性，即在样本无限多的情况下，ERM是否能无限逼近$F$中风险最小的分类器$f_F$。</p>
<p>考虑下面的例子：</p>
<blockquote>
<p>例6：设输入空间$X\sim Uni(0,1)$，即在0到1的区间服从均匀分布，，其中当$X\le0.5$时，$Y=0$；当$X\gt 0.5$时，$Y=1$。现有iid样本集$D_n$，$F$不限制范围。</p>
</blockquote>
<p>注意由于例6中输入和输出是确定关系，因此：
$$
f_b(x)=f_F(x)=
\begin{cases}
    0       &amp; \quad \text{if } x \le 0.5\\
    1  &amp; \quad \text{if } x \gt 0.5
\end{cases}
$$
全局风险$R$为0。</p>
<p>同时我们构建这样一个分类器：
$$
f_n(x)=
\begin{cases}
    y_i &amp; \quad \text{if } x \in D_n \text{ and } x=x_i\\
    1  &amp; \quad \text{otherwise}
\end{cases}
$$
也就是说，如果待分类样本$x$出现在样本集，就按样本中那个样本的标签分类，否则分类为1。很明显，这个分类器可以做到$R_{emp}$为0，但是，当放到全部$X$上时，由于$D_n$的大小相对$X$是可以忽略不计的（用测度论的语言来说，$D_n$是一个零测集），所以我们可以认为$f_n$始终输出1，因此$R(f_n)=0.5$。由于$D_n$是可数集合，所以其测度始终为0，因此就算$D_n$无限多，$f_n$也不会收敛到$f_F$，所以，在这个分类器下，ERM不一致。</p>
<p>通过这个反例，我们可以得出结论：<strong>ERM在不加任何限制条件的情况下，不一致</strong>。</p>
<h2 id="43-假设空间与一致收敛">4.3 假设空间与一致收敛</h2>
<p>上面得出结论，ERM不能在不加任何限制的情况下保证一致。那么是否有什么方法，可以限制ERM的某些方面，让ERM一致呢。一个直观感觉是，ERM所面对的假设空间$F$太大了，由于有太多假设可以选择，选择方式过于自由，导致ERM可以直接去拟合样本集。那么如果限制假设空间，会有什么效果吗？</p>
<h3 id="431-单一函数假设空间与大数定律">4.3.1 单一函数假设空间与大数定律</h3>
<p>我们先可以考虑一个最极端的情况，如果$F$中仅有一个函数$f$，此时，任何学习算法都显然一致，因为此时假设空间中只有一个选择，所以无论何时$f_n=f_F=f$。虽然这样一个假设空间看起来毫无作用，但是我们可以借此分析一个问题：当$n\rightarrow\infty$时，$R_{emp}(f)\rightarrow R(f)$吗？换言之，当样本增多时，$f$的经验风险是否可以无限逼近真实风险？</p>
<p>为了回答这个问题，先引入统计学中一个重要的定理：大数定律。</p>
<hr>
<p><strong>大数定律</strong></p>
<p>设有随机变量$\xi$，以及$\xi$的iid抽样$D_n=\{\xi_1,\xi_2,...,\xi_n\}$。当$n\rightarrow \infty$时，样本均值依概率收敛于$\xi$的期望，用数学语言表述就是，对于任意$\epsilon \gt 0$：
$$
P(|\frac{1}{n}\sum_{i=1}^n{\xi_i}-E(\xi)|&gt;\epsilon)\rightarrow 0, \text{ when } n\rightarrow \infty
$$</p>
<hr>
<p>如果考察上面的表示，发现在$f$固定的情况下，$R(f)$是$L$的期望，而$R_{emp}(f)$是$L$的样本均值，因此根据大数定律可以直接得出结论：<strong>在$f$固定时，$R_{emp}(f)$依概率收敛到$R(f)$</strong>。</p>
<h3 id="432-一致收敛（uniform-convergence）">4.3.2 一致收敛（Uniform Convergence）</h3>
<p>通过以上分析可以看到，如果是单一函数，经验风险是依概率收敛到真实风险的，而当$F$中有多个函数时，由于$f_n$是一个依赖于样本集的非固定函数，因此大数定律失效。</p>
<p>那么有没有办法将大数定律推广到假设空间是多函数情况的呢，考虑下面定义：
$$
\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|
$$
这个式子表示$F$中所有函数真实风险和经验风险误差的上界。由于是上界，我们显然有：
$$
|R(f)-R_{emp}(f)|\le\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)| \\
P(|R(f_n)-R_{emp}(f_n)|\ge\epsilon)\le P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\ge\epsilon)
$$
显然如果上式子右边依概率收敛，则左边也依概率收敛。下面将上式和$F$的一致性关联起来：
$$
\begin{align}
|R(f_n)-R(f_F)| &= R(f_n)-R(f_F) \\
&= R(f_n) +(-R_{emp}(f_n)+R_{emp}(f_n)-R_{emp}(f_F)+R_{emp}(f_F)) -R(f_F) \\
&amp;\le (R(f_n)-R_{emp}(f_n))+(R(f_F)-R_{emp}(f_F)) \\
&amp;\le 2\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|
\end{align}
$$
综上可得：
$$
P(|R(f_n)-R(f_F)|\ge\epsilon)\le P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\ge\epsilon/2)
$$
如果右边依概率收敛，则左边必然依概率收敛，而左边依概率收敛，则意味着一致。</p>
<p>如果随着$n\rightarrow\infty$，$P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\ge\epsilon)\rightarrow 0$，我们说假设空间$F$是一致收敛（Uniform convergence）的。</p>
<p>由上，我们得出重要结论：<strong>如果假设空间$F$一致收敛，则ERM学习在$F$上是一致的</strong>。</p>
<h1 id="5-假设空间度量">5 假设空间度量</h1>
<p>从上面分析我们可以知道，若要使用ERM，则必须保证假设空间$F$一致收敛，否则无法保证ERM的一致性。那么现在我们面对的问题就变成了，如何选择或从数学上证明一个假设空间是一致收敛的呢？</p>
<h2 id="51-有限假设空间">5.1 有限假设空间</h2>
<p>我们先讨论假设空间有限的情况。设$F=\{f_1,f_2,...,f_m\}$是一个具有$m$个假设函数的有限假设空间。这：
$$
P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\ge \epsilon)\le\sum_{i=1}^mP(|R(f_i)-R_{emp}(f_i)|\ge \epsilon)
$$
以上式子的意义是，由于$F$中任意一个假设的经验风险与实际风险的误差大于$\epsilon$就会导致$F$的误差上限大于$\epsilon$，所以右侧是比左侧更可能的发生的事件，所以右侧的概率大于左侧。</p>
<p>我们现在不加证明给出一个定理：Chernoff bound：</p>
<hr>
<p><strong>Chernoff bound</strong></p>
<p>如果$\xi$是一个随机变量，且取值范围为$[0,1]$，则对于$\xi$的iid样本集$\{\xi_1,\xi_2,...,\xi_n\}$，有下面不等式：
$$
P(|\frac{1}{n}\sum_{i=1}^n\xi_i-E(\xi)|\ge\epsilon)\le2\exp(-2n\epsilon^2)
$$</p>
<hr>
<p>Chernoff bound相比大数定律给出了一个定量的上界。由Chernoff bound可以得出：
$$
P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\ge \epsilon)\le\sum_{i=1}^mP(|R(f_i)-R_{emp}(f_i)|\ge \epsilon) \le 2m\ exp(-2n\epsilon^2)
$$
和明显，当$n\rightarrow\infty$时，右侧收敛到0，这蕴含着$F$一致收敛，因此：<strong>有限的假设空间上，ERM是一致的</strong>。</p>
<h2 id="52-无限假设空间">5.2 无限假设空间</h2>
<p>一般来说，我们现实中很少会使用有限假设空间，因此，我们需要继续分析假设空间无限的情况，有什么方法可以保证其一致收敛。</p>
<h3 id="521-symmetrization">5.2.1 Symmetrization</h3>
<p>在继续分析之前，我们先来处理一个十分讨厌的问题。如果仔细看上面的分析就会发现，我们一直带着一个很烦人的东西$R(f)$，为什么这东西烦人呢，因为由于实际中我们无法知道联合概率分布，所以自然也无法计算出真实风险$R(f)$，所以如果继续带着这么一个无法观测的东西，总觉得是颗雷。所以我们先把这东西用什么可以观测的量代替掉。</p>
<p>这个还真可以做到，需要通过一个叫Symmetrization的技巧。简单来说，Symmetrization的意思是，如果我们除了$D_n$，还有一组独立于$D_n$且同样大小的iid抽样$D_n&#39;$，我们定义其上的经验风险是$R_{emp}&#39;(f)$，则有如下定理：</p>
<p>若$m\epsilon^2\gt 2$，则：
$$
P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\gt\epsilon)\le2P(\mathop{sup}_{f\in F}|R_{emp}(f)-R_{emp}&#39;(f)|\gt\epsilon/2)
$$
由于证明比较繁琐，我们不去证明这个不等式。但是我们可以看出上式的价值，就是我们不再依赖不可观测的$R(f)$了，而只依赖可以观测的经验风险。所以如果我们能找出什么条件使得$\mathop{sup}_{f\in F}{|R_{emp}(f)-R_{emp}&#39;(f)|}$依概率收敛，则大功告成。现在我们进行这一步。</p>
<h3 id="522-shattering-coefficient">5.2.2 Shattering Coefficient</h3>
<p>为什么只依赖经验风险是非常重要的呢，因为我们需要注意这样一个非常重要的事实：<strong>即使$F$是无限的，但是对于特定的$n$，经验风险的取值是有限的</strong>。</p>
<p>什么意思呢，考虑一个二分类问题，给出一个有n个样本的样本集，不论你的$F$中有多少函数，最终输出最多只能有$2^n$种可能（每个样本输出要么0，要么1），所以，如果只考虑经验风险，任何$F$在面对n个样本时，在ERM评价下其等价于一个<strong>最多</strong>有$2^n$个假设函数的有限假设空间。</p>
<p>我们量化一下这个度量，对于假设空间$F$，我们定义一个函数：
$$
\mathcal{N}(F,n)=\text{当样本数为n时，F所能产生的不同分类结果的数量}
$$
这个函数我们称为$F$的shattering coefficient，下面举例子直观说明一下它的含义。</p>
<blockquote>
<p>例7：设一个输入空间在$[0,1]$上的二分类问题。有这样一个模型：它选择一个点$\theta\in [0,1]$，将区间分为$[0,\theta]$和$(\theta,1]$两部分，然后将两个部分分别作为一个类别。这个模型的shattering coefficient是什么？</p>
</blockquote>
<p>简单来说，这个模型就是将输入空间一分为二，一边赋值为0，一边赋值为1，因为$\theta$有无数种选择，所以$F$是一个无限集合。</p>
<p>另外可以看出，基于上面模型分类时，能输出几种分类结果，仅与样本点的标签和相对排布有关，与具体位置无关。当有n个二值样本点时，在一维空间里有$2^n$种排布方式。显然在$n\le 2$时，是可以完美分开的，所以$\mathcal{N}(F,1)=2$，$\mathcal{N}(F,2)=4$。现在考察$n\gt 2$的情况：</p>
<ul>
<li>当$n=3$时，有8种排布方式，其中ooo，oox，oxx，xxx，xxo，xoo可以由$F$输出，但是oxo和xox两种情况是无法输出的，因此$$\mathcal{N}(F,3)=6$$</li>
<li>当$n=4$时，有16种排布方式，其中oooo，ooox，ooxx，oxxx，xxxx，xxxo，xxoo，xooo可以由$F$输出，其他情况无法输出的，因此$$\mathcal{N}(F,4)=8$$</li>
</ul>
<p>我们一般化考虑一下，当样本个数为n时，有$2^n$中排布可能，但是$F$中假设函数可以输出的排布，一定是可以找到某一点，左边全是x，右边全是o；或者左边全是o，右边全是x。可以这样思考有多少可能：</p>
<ol>
<li>首先n个样本全是oo...o，这是一个输出可能</li>
<li>从右边开始，依次将每个位置的样本由o变为x，经过n次，变为xx..x，这是n种可能输出</li>
<li>再从右边开始，依次将x变成o，经过n次，变为oo..o，也是n种输出可能，但是由于全是o的情况在上面已经提到了，所以这里有n-1种新的输出可能</li>
<li>将上面可能输出相加$1+n+n-1=2n$</li>
</ol>
<p>因此我们可得，对于例7的模型，$\mathcal{N}(F,n)=2n$</p>
<h3 id="523-一致收敛上界">5.2.3 一致收敛上界</h3>
<p>有了上面的铺垫，我们可以推导一般情况下ERM一致收敛的条件了。
$$
\begin{align}
&amp;P(\mathop{sup}_{f\in F}|R(f)-R_{emp}(f)|\gt\epsilon) \\
&amp;\le 2P(\mathop{sup}_{f\in F}|R_{emp}(f)-R_{emp}&#39;(f)|\gt\epsilon/2) \\
&= 2P(\mathop{sup}_{f\in F_{2^{2n}}}|R_{emp}(f)-R_{emp}&#39;(f)|\gt\epsilon/2) \\
&amp;\le 2\mathcal{N}(F,2n)exp(-n\epsilon^2/4)
\end{align} \\
$$</p>
<p>这个推导稍微有点复杂，下面说明一下：</p>
<ol>
<li>第一步推导是上面提到的Symmetrization</li>
<li>第二部推导是基于在两个容量为n的样本集上，其经验风险差值的输出只有$2^{2n}$种可能，将$F$等价为一个容量为$2^{2n}$的有限假设空间</li>
<li>第三步推论基于有限假设空间的Chernoff bound给出</li>
</ol>
<p>至此，我们得到了ERM一致的一个一般性上界条件：<strong>若随着$n\rightarrow \infty$，$2\mathcal{N}(F,2n)exp(-n\epsilon^2/4)\rightarrow 0$，则ERM是一致的</strong>。</p>
<p>看几个例子：</p>
<ul>
<li>上面提到例7中，$\mathcal{N}(F,2n)=4n$，代入上界，得到$8nexp(-n\epsilon^2/4)$，显然这个是收敛到0的，所以例7在ERM下是一致的</li>
<li>考虑$\mathcal{N}(F,n)\le (2n)^k$，其中$k$是常数。此时，上界为$2exp(klog(2n)-n\epsilon^2/4)$，显然是收敛到0的。所以<strong>如果$\mathcal{N}(F,2n)$随着n的增长呈多项式级别增长，则ERM是一致的</strong></li>
<li>考虑$F$可以任意取函数，也就是$\mathcal{N}(F,2n)=2^{2n}$，此时上界为$2exp(n(2log(2)-\epsilon^2/4)$，这个上界并不收敛到0。但是此时不能贸然说这个$F$就不一致，因为上界收敛只是一致的充分条件，而不是必要条件。如果用更多的数学技巧，我们可以得出一致的一个必要条件：$log\mathcal{N}(F,n/)/n\rightarrow 0$，由于$log(2^n)/n=nlog2/n=log2$不收敛到0，所以如果$F$取全体可能的假设时，ERM是不一致的</li>
</ul>
<h3 id="524-vc维">5.2.4 VC维</h3>
<p>上面的shattering coefficient可以看成对假设空间容量的一种度量，除了shattering coefficient外，还有一些其他的度量方式，可以给出ERM一致的充分条件。我们这里介绍其中最知名的一个，叫做VC维（VC Dimension）。</p>
<p>简单来说，一个假设空间的$F$的VC维是一个非负整数整数$d$，其意义是，我们可以构造一个容量为$d$的样本，使得$F$中的函数可以输出$2^d$种不同结果，但是无法构造一个容量为$d+1$的样本，使得$F$中的函数可以输出$2^{d+1}$种不同结果。</p>
<p>举个例子。考虑二维空间的线性分类器，当样本容量为3时，我们可以这样排布样本，使得8种情况都可以被分开：</p>
<p><img src="../uploads/pictures/statistical-learning-theory/slt03.png" alt="slt03"></p>
<p>注意，这里我们不需要对于所有容量为3的样本都可以完全分开（若三个样本共线，则存在线性分类器分不开的情况），我们只需要存在这样一组样本即可。</p>
<p>但是，当样本容量为4时，则无法构造一个排布令16种情况都可以被分开，因为无论如何排不，都会遇到类似如下的情况：</p>
<p><img src="../uploads/pictures/statistical-learning-theory/slt04.png" alt="slt04"></p>
<p>无法被一条线分开。</p>
<p>所以，二维平面上线性分类器的VC维为3。</p>
<p>基于VC维，我们可以给出这样一个ERM一致的充分必要条件：<strong>ERM对于假设空间$F$一致，当且仅当$F$的VC维有限</strong>。</p>
<p>由于是充要条件，VC维往往可以用来判断一个ERM是否有效。例如可以证明，对于任何有限的维数，线性分类器的VC维都是有限的，所以线性分类器总是ERM一致。这就为我们使用ERM准则训练线性模型做分类找到了理论依据。例如，逻辑回归就是一种线性分类器，其训练准则就是ERM。</p>
<h1 id="6-天下没有免费午餐定理">6 天下没有免费午餐定理</h1>
<p>TBD</p>

        </div>
        

    </div>
</div>
        <div id="footer">
            <div id="footer-inner">
                <p id="copyright">Copyright (c) 2011-2018 CodingLabs 本博客内容采用<a href="http://creativecommons.org/licenses/by/3.0/cn/" target="_blank">知识共享署名 3.0 中国大陆许可协议</a>进行许可</p>
            </div>
        </div>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [['$','$'], ['\\(','\\)']],
                    processEscapes: true
                }
            });
        </script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </body>
</html>

